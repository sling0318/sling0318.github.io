<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="theme-color" content="#222"><meta name="generator" content="Hexo 5.4.1">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">



<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.0.0/css/all.min.css" integrity="sha256-jTIdiMuX/e3DGJUGwl3pKSxuc6YOuqtJYkM0bGQESA4=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/animate.css@3.1.1/animate.min.css" integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin="anonymous">

<script class="next-config" data-name="main" type="application/json">{"hostname":"example.com","root":"/","images":"/images","scheme":"Pisces","darkmode":false,"version":"8.10.1","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12},"copycode":false,"bookmark":{"enable":false,"color":"#222","save":"auto"},"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"stickytabs":false,"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"prism":false,"i18n":{"placeholder":"搜索...","empty":"没有找到任何搜索结果：${query}","hits_time":"找到 ${hits} 个搜索结果（用时 ${time} 毫秒）","hits":"找到 ${hits} 个搜索结果"}}</script><script src="/js/config.js"></script>

    <meta name="description" content="sci-2。使用部分卸载。考虑的时蜂窝网络的环境，使用多智能体强化学习(DRL)的方法最小化延迟。为了降低训练过程的计算复杂性和开销，引入了联邦学习，设计了一个联邦DRL方案。">
<meta property="og:type" content="article">
<meta property="og:title" content="[文献阅读] Multi-Agent Deep Reinforcement Learning for Computation Offloading and Interference Coordination in Small Cell Networks 用于小蜂窝网络计算卸载和干扰协调的多智能体深度强化学习">
<meta property="og:url" content="http://example.com/2022/05/21/[%E6%96%87%E7%8C%AE%E9%98%85%E8%AF%BB]Multi-Agent%20Deep%20Reinforcement%20Learning%20for%20Computation%20Offloading%20and%20Interference%20Coordination%20in%20Small%20Cell%20Networks/index.html">
<meta property="og:site_name" content="生灵">
<meta property="og:description" content="sci-2。使用部分卸载。考虑的时蜂窝网络的环境，使用多智能体强化学习(DRL)的方法最小化延迟。为了降低训练过程的计算复杂性和开销，引入了联邦学习，设计了一个联邦DRL方案。">
<meta property="og:locale" content="zh_CN">
<meta property="article:published_time" content="2022-05-21T07:13:33.000Z">
<meta property="article:modified_time" content="2022-05-26T08:31:08.240Z">
<meta property="article:author" content="Ciallo～(∠・ω&lt; )⌒☆​">
<meta property="article:tag" content="强化学习">
<meta property="article:tag" content="文献阅读">
<meta property="article:tag" content="边缘计算">
<meta property="article:tag" content="计算卸载">
<meta property="article:tag" content="联邦学习">
<meta name="twitter:card" content="summary">


<link rel="canonical" href="http://example.com/2022/05/21/[%E6%96%87%E7%8C%AE%E9%98%85%E8%AF%BB]Multi-Agent%20Deep%20Reinforcement%20Learning%20for%20Computation%20Offloading%20and%20Interference%20Coordination%20in%20Small%20Cell%20Networks/">



<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":false,"isPost":true,"lang":"zh-CN","comments":true,"permalink":"http://example.com/2022/05/21/[%E6%96%87%E7%8C%AE%E9%98%85%E8%AF%BB]Multi-Agent%20Deep%20Reinforcement%20Learning%20for%20Computation%20Offloading%20and%20Interference%20Coordination%20in%20Small%20Cell%20Networks/","path":"2022/05/21/[文献阅读]Multi-Agent Deep Reinforcement Learning for Computation Offloading and Interference Coordination in Small Cell Networks/","title":"[文献阅读] Multi-Agent Deep Reinforcement Learning for Computation Offloading and Interference Coordination in Small Cell Networks 用于小蜂窝网络计算卸载和干扰协调的多智能体深度强化学习"}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title>[文献阅读] Multi-Agent Deep Reinforcement Learning for Computation Offloading and Interference Coordination in Small Cell Networks 用于小蜂窝网络计算卸载和干扰协调的多智能体深度强化学习 | 生灵</title>
  





  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
</head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏" role="button">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <p class="site-title">生灵</p>
      <i class="logo-line"></i>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a></li><li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签</a></li><li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>目录</a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a></li>
  </ul>
</nav>




</div>
        
  
  <div class="toggle sidebar-toggle" role="button">
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
  </div>

  <aside class="sidebar">

    <div class="sidebar-inner sidebar-nav-active sidebar-toc-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
            <div class="post-toc animated"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#%E6%91%98%E8%A6%81%E4%B8%8E%E4%B8%BB%E8%A6%81%E8%B4%A1%E7%8C%AE"><span class="nav-number">1.</span> <span class="nav-text">摘要与主要贡献</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E7%B3%BB%E7%BB%9F%E6%A8%A1%E5%9E%8B"><span class="nav-number">2.</span> <span class="nav-text">系统模型</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%94%AF%E6%8C%81%E8%BE%B9%E7%BC%98%E6%99%BA%E8%83%BD%E7%9A%84%E5%B0%8F%E5%9E%8B%E8%9C%82%E7%AA%9D%E7%BD%91%E7%BB%9C"><span class="nav-number">2.1.</span> <span class="nav-text">支持边缘智能的小型蜂窝网络</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E9%80%9A%E4%BF%A1%E6%A8%A1%E5%9E%8B"><span class="nav-number">2.2.</span> <span class="nav-text">通信模型</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E8%AE%A1%E7%AE%97%E6%A8%A1%E5%9E%8B"><span class="nav-number">2.3.</span> <span class="nav-text">计算模型</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%9C%AC%E5%9C%B0%E8%AE%A1%E7%AE%97"><span class="nav-number">2.3.1.</span> <span class="nav-text">本地计算</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E8%BE%B9%E7%BC%98%E8%AE%A1%E7%AE%97"><span class="nav-number">2.3.2.</span> <span class="nav-text">边缘计算</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E5%A4%9A%E6%99%BA%E8%83%BD%E4%BD%93DRL%E8%AE%A1%E7%AE%97%E5%8D%B8%E8%BD%BD%E5%92%8C%E8%B5%84%E6%BA%90%E5%88%86%E9%85%8D%E6%A8%A1%E5%9E%8B"><span class="nav-number">3.</span> <span class="nav-text">多智能体DRL计算卸载和资源分配模型</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%A4%9A%E6%99%BA%E8%83%BD%E4%BD%93%E7%8E%AF%E5%A2%83"><span class="nav-number">3.1.</span> <span class="nav-text">多智能体环境</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%9F%BA%E4%BA%8EMARL%E7%9A%84%E8%AE%A1%E7%AE%97%E5%8D%B8%E8%BD%BD%E4%BA%8E%E8%B5%84%E6%BA%90%E5%88%86%E9%85%8D%E6%96%B9%E6%A1%88"><span class="nav-number">3.2.</span> <span class="nav-text">基于MARL的计算卸载于资源分配方案</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#MADRL%E7%AE%97%E6%B3%95"><span class="nav-number">3.3.</span> <span class="nav-text">MADRL算法</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E8%81%94%E9%82%A6DRL%E8%AE%A1%E7%AE%97%E5%8D%B8%E8%BD%BD"><span class="nav-number">4.</span> <span class="nav-text">联邦DRL计算卸载</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E9%97%AE%E9%A2%98%E8%A1%A8%E8%BF%B0"><span class="nav-number">4.1.</span> <span class="nav-text">问题表述</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E8%81%94%E9%82%A6MARL%E6%A1%86%E6%9E%B6"><span class="nav-number">4.2.</span> <span class="nav-text">联邦MARL框架</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%AE%97%E6%B3%95"><span class="nav-number">4.3.</span> <span class="nav-text">算法</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E5%8E%9F%E6%96%87%E5%9C%B0%E5%9D%80"><span class="nav-number">5.</span> <span class="nav-text">原文地址</span></a></li></ol></div>
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author site-overview-item animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="Ciallo～(∠・ω< )⌒☆​"
      src="/images/dh.png">
  <p class="site-author-name" itemprop="name">Ciallo～(∠・ω< )⌒☆​</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap site-overview-item animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
        <a href="/archives/">
          <span class="site-state-item-count">12</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
          <a href="/categories/">
        <span class="site-state-item-count">3</span>
        <span class="site-state-item-name">目录</span></a>
      </div>
      <div class="site-state-item site-state-tags">
          <a href="/tags/">
        <span class="site-state-item-count">8</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>



        </div>
      </div>
    </div>
  </aside>
  <div class="sidebar-dimmer"></div>


    </header>

    
  <div class="back-to-top" role="button" aria-label="返回顶部">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


    <div class="main-inner post posts-expand">


  


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/2022/05/21/[%E6%96%87%E7%8C%AE%E9%98%85%E8%AF%BB]Multi-Agent%20Deep%20Reinforcement%20Learning%20for%20Computation%20Offloading%20and%20Interference%20Coordination%20in%20Small%20Cell%20Networks/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/dh.png">
      <meta itemprop="name" content="Ciallo～(∠・ω< )⌒☆​">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="生灵">
      <meta itemprop="description" content="">
    </span>
    
    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="[文献阅读] Multi-Agent Deep Reinforcement Learning for Computation Offloading and Interference Coordination in Small Cell Networks 用于小蜂窝网络计算卸载和干扰协调的多智能体深度强化学习 | 生灵">
      <meta itemprop="description" content="sci-2。使用部分卸载。考虑的时蜂窝网络的环境，使用多智能体强化学习(DRL)的方法最小化延迟。为了降低训练过程的计算复杂性和开销，引入了联邦学习，设计了一个联邦DRL方案。">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          [文献阅读] Multi-Agent Deep Reinforcement Learning for Computation Offloading and Interference Coordination in Small Cell Networks 用于小蜂窝网络计算卸载和干扰协调的多智能体深度强化学习
        </h1>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2022-05-21 15:13:33" itemprop="dateCreated datePublished" datetime="2022-05-21T15:13:33+08:00">2022-05-21</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2022-05-26 16:31:08" itemprop="dateModified" datetime="2022-05-26T16:31:08+08:00">2022-05-26</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">收录于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E6%96%87%E7%8C%AE%E9%98%85%E8%AF%BB/" itemprop="url" rel="index"><span itemprop="name">文献阅读</span></a>
        </span>
    </span>

  
</div>

            <div class="post-description">sci-2。使用部分卸载。考虑的时蜂窝网络的环境，使用多智能体强化学习(DRL)的方法最小化延迟。为了降低训练过程的计算复杂性和开销，引入了联邦学习，设计了一个联邦DRL方案。</div>
        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
        <h1 id="摘要与主要贡献"><a href="#摘要与主要贡献" class="headerlink" title="摘要与主要贡献"></a>摘要与主要贡献</h1><ul>
<li>期刊：IEEE TRANSACTIONS ON VEHICULAR TECHNOLOGY.2021.CCF-A</li>
</ul>
<blockquote>
<p>缩写：小型蜂窝基站(SBS) 用户设备(UE)</p>
</blockquote>
<p>主要研究小蜂窝网络的分布式智能计算卸载和干扰协调方案。重点研究了基于MARL的多用户多小区网络中计算卸载、信道分配、功率控制和计算资源分配的联合优化问题。</p>
<ul>
<li>论文贡献：</li>
<li>将联合计算卸载和干扰协调问题作为多智能体DRL问题，最小化系统能耗。</li>
<li>提出了一种基于分布式多智能体DRL的计算卸载和资源分配方法。</li>
<li>为了减少训练过程的计算复杂度和开销，提出了一种基于DRL的联邦计算卸载和资源分配方法。它只需要SBS代理共享其模型参数而不是本地训练数据。</li>
</ul>
<hr>
<h1 id="系统模型"><a href="#系统模型" class="headerlink" title="系统模型"></a>系统模型</h1><h2 id="支持边缘智能的小型蜂窝网络"><a href="#支持边缘智能的小型蜂窝网络" class="headerlink" title="支持边缘智能的小型蜂窝网络"></a>支持边缘智能的小型蜂窝网络</h2><p><img src = "
/Multi-Agent Deep Reinforcement Learning for Computation Offloading and Interference Coordination in Small Cell Networks/
图片1.png
" height = 400></p>
<ul>
<li><p>如图1所示，由随机分布的SBS和非随机分布的UE组成。SBS配备了MEC服务器，表示为$\mathcal{M}={0,1,2,…M}$，用户和SBS的集合为$m \in \mathcal{M}, \mathcal{N}_m={1,2,…,N_m}, \sum_m| \mathcal{N}_m |=N$。</p>
</li>
<li><p>边缘服务器m中的用户n的任务可以表示为$Task_n^{(m)}=\{d_n^{(m)}, \omega_n^{(m)},\tilde{T}_n^{(m)}\}$, <strong>$d_n^{(m)}$</strong> 表示任务的大小， <strong>$\omega_n^{(m)}$</strong> 表示完成计算任务所需要的计算能力(CPU周期)， <strong>$\tilde{T}_n^{(m)}$</strong> 表示UE可以容忍的最大延迟。</p>
</li>
<li><p>考虑的是部分卸载。</p>
</li>
</ul>
<h2 id="通信模型"><a href="#通信模型" class="headerlink" title="通信模型"></a>通信模型</h2><ul>
<li><p>定义 <strong>$\alpha_n^{(m)} \in [0,1]$</strong> 为卸载比例。具体的， <strong>$\alpha_n^{(m)}d_n^{(m)}$</strong> 为在边缘服务器上计算的任务大小，<strong>$(1-\alpha_n^{(m)})d_n^{(m)}$</strong> 为在本地计算的大小。</p>
</li>
<li><p>K表示系统中的正交信道，不同区域之间存在同频干扰，在每个区域内，采用正交频分多址方法，即不会有干扰。定义 <strong>$b_{n,k}^{m} \in \{0,1\}$</strong> 作为m的信道指示器。$b_{n,k}^{(m)}=1$表示m将信道k分配给m，反之$b_{n,k}^{(m)}=0$。SBS的信道分配决策满足约束$\sum_{n \in \mathcal{N}_m}b_{n,k}^{(m)} ≤1,\forall k \in \mathcal{K}, m \in \mathcal{M}$。此外，假设每个UE卸载任务时可以在最长的信道进行分配，即$\sum_{k \in \mathcal{K}_m}b_{n,k}^{(m)} ≤1,\forall n \in \mathcal{N}_m, m \in \mathcal{M}$。</p>
</li>
<li>定义 <strong>$p_n^{(m)} \in [0,P_n^{(m),max}]$</strong> 为n向m的传输功率，$P_n^{(m),max}$是m的最大传输功率。因此，当用户n卸载任务到边缘服务器m上时，在信道k上的上行链路的速率为：</li>
</ul>
<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><msubsup><mi>r</mi><mrow><mi>n</mi><mo>,</mo><mi>k</mi></mrow><mrow><mo stretchy="false">(</mo><mi>m</mi><mo stretchy="false">)</mo></mrow></msubsup><mo>=</mo><mi>B</mi><msub><mi>log</mi><mrow><mn>2</mn></mrow></msub><mo data-mjx-texclass="NONE">⁡</mo><mrow data-mjx-texclass="INNER"><mo data-mjx-texclass="OPEN">(</mo><mn>1</mn><mo>+</mo><mfrac><mrow><msubsup><mi>p</mi><mrow><mi>n</mi></mrow><mrow><mo stretchy="false">(</mo><mi>m</mi><mo stretchy="false">)</mo></mrow></msubsup><msubsup><mi>h</mi><mrow><mi>n</mi><mo>,</mo><mi>k</mi></mrow><mrow><mo stretchy="false">(</mo><mi>m</mi><mo stretchy="false">)</mo></mrow></msubsup></mrow><mrow><msub><mi>N</mi><mrow><mn>0</mn></mrow></msub><mi>B</mi><mo>+</mo><msubsup><mi>I</mi><mrow><mi>k</mi></mrow><mrow><mo stretchy="false">(</mo><mi>m</mi><mo stretchy="false">)</mo></mrow></msubsup></mrow></mfrac><mo data-mjx-texclass="CLOSE">)</mo></mrow></math>

<blockquote>
<p>B为信道带宽，$h_{n,k}^{(m)}$为信道增益，$N_0$为噪声，$I_k^{(m)}=\sum_{m^{‘}≠m}\sum_{n^{‘} \in \mathcal{N}_{m^{‘}}},P_{n^{‘}}^{(m){‘}}g_{n{‘},k}^{(m)}$为干扰信道增益。</p>
</blockquote>
<ul>
<li><p>因此，n到m的上行链路速率为：</p>
<center>$U_n^{(m)}=\sum_{k \in \mathcal{K}}b_{n,k}^{(m)}r_{n,k}^{(m)} $</center>
</li>
<li><p>我们可以得知上行链路速率不仅取决于分配的信道和其自身的发射功率，还取决于其他区域中卸载UE的干扰。从系统优化的角度来看，不同区域之间的计算卸载、信道分配和功率控制的协调对于缓解小区间干扰从而提高卸载性能至关重要。</p>
</li>
</ul>
<h2 id="计算模型"><a href="#计算模型" class="headerlink" title="计算模型"></a>计算模型</h2><h3 id="本地计算"><a href="#本地计算" class="headerlink" title="本地计算"></a>本地计算</h3><ul>
<li><p>设CPU频率为 <strong>$C_n^{(m)}$</strong> ,本地计算的时间为：</p>
<center>$LocT_m^{(m)}=(1-\alpha_n^{(m)})\omega_n^{(m)}/C_n^{(m)} $</center>
</li>
<li><p>$\varepsilon_n^{(m)}$为能耗系数，本地计算的能耗为：</p>
<center>$LocE_n^{(m)}=(1-\alpha_n^{(m)})\omega_n^{(m)} \varepsilon_n^{(m)} $</center>


</li>
</ul>
<h3 id="边缘计算"><a href="#边缘计算" class="headerlink" title="边缘计算"></a>边缘计算</h3><ul>
<li>给定卸载比例 $\alpha_n^{(m)}$，可以得到传输到SBS的<strong>传输时延</strong>和<strong>传输能耗</strong>为：<center>$OffT_n^{(m),tr}=\alpha_n^{(m)}d_n^{(m)}/U_n^{(m)} $</center>
<center>$OffE_n^{(m),tr}=p_n^{(m)}\alpha_n^{(m)}d_n^{(m)}/U_n^{(m)} $</center>


</li>
</ul>
<ul>
<li>SBS上边缘服务器的计算能力为 <strong>$\bar{C}^{(m)}$</strong> 。对于卸载到m的移动设备n，定义 <strong>$f_n^{(m)} \in (0,1]$</strong> 为n分配到的资源。因此在边缘服务上的<strong>计算时延</strong>和<strong>计算能耗</strong>为：<center>$OffT_n^{(m),ex}=\alpha_n^{(m)} \omega_n^{(m)}/f_n^{(m)} \bar{C}^{(m)} $</center>
<center>$OffE_n^{(m),ex}=\alpha_n^{(m)} \omega_n^{(m)} \bar{\varepsilon}^{(m)} $</center>

</li>
</ul>
<blockquote>
<p>其中$\bar{\varepsilon}^{(m)} $为能耗系数。</p>
</blockquote>
<ul>
<li>可以得到总时延和总能耗为：<center>$OffT_n^{(m)}=OffT_n^{(m),tr}+OffT_n^{(m),ex}$</center>
<center>$OffE_n^{(m)}=OffE_n^{(m),tr}+OffE_n^{(m),ex}$</center>

</li>
</ul>
<hr>
<h1 id="多智能体DRL计算卸载和资源分配模型"><a href="#多智能体DRL计算卸载和资源分配模型" class="headerlink" title="多智能体DRL计算卸载和资源分配模型"></a>多智能体DRL计算卸载和资源分配模型</h1><ul>
<li>将一个调度周期设为一个时隙，每个时隙的计算卸载和资源分配问题表述如下：<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mtable displaystyle="true" columnalign="right left" columnspacing="0em" rowspacing="3pt"><mtr><mtd></mtd><mtd><mrow><mi mathvariant="normal">P</mi></mrow><mn>0</mn><mo>:</mo><munder><mo data-mjx-texclass="OP" movablelimits="true">min</mo><mrow><mi mathvariant="bold-italic">α</mi><mo stretchy="false">[</mo><mi>t</mi><mo stretchy="false">]</mo><mo>,</mo><mrow><mi mathvariant="bold">b</mi></mrow><mo stretchy="false">[</mo><mi>t</mi><mo stretchy="false">]</mo><mo>,</mo><mrow><mi mathvariant="bold">p</mi></mrow><mo stretchy="false">[</mo><mi>t</mi><mo stretchy="false">]</mo><mo>,</mo><mrow><mi mathvariant="bold">f</mi></mrow><mo stretchy="false">[</mo><mi>t</mi><mo stretchy="false">]</mo></mrow></munder><munder><mo data-mjx-texclass="OP">∑</mo><mrow><mi>m</mi></mrow></munder><munder><mo data-mjx-texclass="OP">∑</mo><mrow><mi>n</mi></mrow></munder><mrow data-mjx-texclass="INNER"><mo data-mjx-texclass="OPEN">(</mo><msubsup><mi>LocE</mi><mrow><mi>n</mi></mrow><mrow><mo stretchy="false">(</mo><mi>m</mi><mo stretchy="false">)</mo></mrow></msubsup><mo data-mjx-texclass="NONE">⁡</mo><mo stretchy="false">[</mo><mi>t</mi><mo stretchy="false">]</mo><mo>+</mo><mi>O</mi><mi>f</mi><mi>f</mi><msubsup><mi>E</mi><mrow><mi>n</mi></mrow><mrow><mo stretchy="false">(</mo><mi>m</mi><mo stretchy="false">)</mo></mrow></msubsup><mo stretchy="false">[</mo><mi>t</mi><mo stretchy="false">]</mo><mo data-mjx-texclass="CLOSE">)</mo></mrow></mtd></mtr><mtr><mtd></mtd><mtd><mi>s</mi><mo>.</mo><mi>t</mi><mo>.</mo></mtd></mtr><mtr><mtd></mtd><mtd><mrow><mi mathvariant="normal">C</mi></mrow><mn>1</mn><mo>:</mo><mo data-mjx-texclass="OP" movablelimits="true">max</mo><mrow data-mjx-texclass="INNER"><mo data-mjx-texclass="OPEN">{</mo><mi>Loc</mi><mo data-mjx-texclass="NONE">⁡</mo><msubsup><mi>T</mi><mrow><mi>n</mi></mrow><mrow><mo stretchy="false">(</mo><mi>m</mi><mo stretchy="false">)</mo></mrow></msubsup><mo stretchy="false">[</mo><mi>t</mi><mo stretchy="false">]</mo><mo>,</mo><mi>O</mi><mi>f</mi><mi>f</mi><msubsup><mi>T</mi><mrow><mi>n</mi></mrow><mrow><mo stretchy="false">(</mo><mi>m</mi><mo stretchy="false">)</mo></mrow></msubsup><mo stretchy="false">[</mo><mi>t</mi><mo stretchy="false">]</mo><mo data-mjx-texclass="CLOSE">}</mo></mrow><mo>⩽</mo><msubsup><mrow><mover><mi>T</mi><mo stretchy="false">~</mo></mover></mrow><mrow><mi>n</mi></mrow><mrow><mo stretchy="false">(</mo><mi>m</mi><mo stretchy="false">)</mo></mrow></msubsup><mo>,</mo><mi mathvariant="normal">∀</mi><mi>m</mi><mo>,</mo><mi>n</mi></mtd></mtr><mtr><mtd></mtd><mtd><mi>C</mi><mn>2</mn><mo>:</mo><msubsup><mi>α</mi><mrow><mi>n</mi></mrow><mrow><mo stretchy="false">(</mo><mi>m</mi><mo stretchy="false">)</mo></mrow></msubsup><mo stretchy="false">[</mo><mi>t</mi><mo stretchy="false">]</mo><mo>∈</mo><mo stretchy="false">[</mo><mn>0</mn><mo>,</mo><mn>1</mn><mo stretchy="false">]</mo><mo>,</mo><mi mathvariant="normal">∀</mi><mi>n</mi><mo>,</mo><mi>m</mi></mtd></mtr><mtr><mtd></mtd><mtd><mrow><mi mathvariant="normal">C</mi></mrow><mn>3</mn><mo>:</mo><msubsup><mi>b</mi><mrow><mi>n</mi><mo>,</mo><mi>k</mi></mrow><mrow><mo stretchy="false">(</mo><mi>m</mi><mo stretchy="false">)</mo></mrow></msubsup><mo stretchy="false">[</mo><mi>t</mi><mo stretchy="false">]</mo><mo>∈</mo><mo fence="false" stretchy="false">{</mo><mn>0</mn><mo>,</mo><mn>1</mn><mo fence="false" stretchy="false">}</mo><mo>,</mo><mi mathvariant="normal">∀</mi><mi>n</mi><mo>,</mo><mi>m</mi><mo>,</mo><mi>k</mi></mtd></mtr><mtr><mtd></mtd><mtd><mrow><mi mathvariant="normal">C</mi></mrow><mn>4</mn><mo>:</mo><munder><mo data-mjx-texclass="OP">∑</mo><mrow><mi>k</mi><mo>∈</mo><mrow><mi data-mjx-variant="-tex-calligraphic" mathvariant="script">K</mi></mrow></mrow></munder><msubsup><mi>b</mi><mrow><mi>n</mi><mo>,</mo><mi>k</mi></mrow><mrow><mo stretchy="false">(</mo><mi>m</mi><mo stretchy="false">)</mo></mrow></msubsup><mo stretchy="false">[</mo><mi>t</mi><mo stretchy="false">]</mo><mo>≤</mo><mn>1</mn><mo>,</mo><mi mathvariant="normal">∀</mi><mi>n</mi><mo>,</mo><mi>m</mi></mtd></mtr><mtr><mtd></mtd><mtd><mi>C</mi><mn>5</mn><mo>:</mo><munder><mo data-mjx-texclass="OP">∑</mo><mrow><mi>n</mi><mo>∈</mo><msub><mrow><mi data-mjx-variant="-tex-calligraphic" mathvariant="script">N</mi></mrow><mrow><mi>m</mi></mrow></msub></mrow></munder><msubsup><mi>b</mi><mrow><mi>n</mi><mo>,</mo><mi>k</mi></mrow><mrow><mo stretchy="false">(</mo><mi>m</mi><mo stretchy="false">)</mo></mrow></msubsup><mo stretchy="false">[</mo><mi>t</mi><mo stretchy="false">]</mo><mo>≤</mo><mn>1</mn><mo>,</mo><mi mathvariant="normal">∀</mi><mi>k</mi><mo>,</mo><mi>m</mi></mtd></mtr><mtr><mtd></mtd><mtd><mi>C</mi><mn>6</mn><mo>:</mo><msubsup><mi>p</mi><mrow><mi>n</mi></mrow><mrow><mo stretchy="false">(</mo><mi>m</mi><mo stretchy="false">)</mo></mrow></msubsup><mo stretchy="false">[</mo><mi>t</mi><mo stretchy="false">]</mo><mo>∈</mo><mrow data-mjx-texclass="INNER"><mo data-mjx-texclass="OPEN">[</mo><mn>0</mn><mo>,</mo><msubsup><mi>P</mi><mrow><mi>n</mi></mrow><mrow><mo stretchy="false">(</mo><mi>m</mi><mo stretchy="false">)</mo><mo>,</mo><mo data-mjx-texclass="OP" movablelimits="true">max</mo></mrow></msubsup><mo data-mjx-texclass="CLOSE">]</mo></mrow><mo>,</mo><mi mathvariant="normal">∀</mi><mi>m</mi><mo>,</mo><mi>n</mi></mtd></mtr><mtr><mtd></mtd><mtd><mi>C</mi><mn>7</mn><mo>:</mo><msubsup><mi>f</mi><mrow><mi>n</mi></mrow><mrow><mo stretchy="false">(</mo><mi>m</mi><mo stretchy="false">)</mo></mrow></msubsup><mo stretchy="false">[</mo><mi>t</mi><mo stretchy="false">]</mo><mo>∈</mo><mo stretchy="false">(</mo><mn>0</mn><mo>,</mo><mn>1</mn><mo stretchy="false">]</mo><mo>,</mo><munder><mo data-mjx-texclass="OP">∑</mo><mrow><mi>n</mi><mo>∈</mo><msub><mrow><mi data-mjx-variant="-tex-calligraphic" mathvariant="script">N</mi></mrow><mrow><mi>m</mi></mrow></msub></mrow></munder><msubsup><mi>f</mi><mrow><mi>n</mi></mrow><mrow><mo stretchy="false">(</mo><mi>m</mi><mo stretchy="false">)</mo></mrow></msubsup><mo stretchy="false">[</mo><mi>t</mi><mo stretchy="false">]</mo><mo>≤</mo><mn>1</mn><mo>,</mo><mi mathvariant="normal">∀</mi><mi>m</mi></mtd></mtr></mtable></math>

</li>
</ul>
<blockquote>
<p>$\pmb{\alpha}[t]={\alpha_n^{(m)}[t]},\pmb{b}[t]={b_{n,k}^{(m)}},\pmb{p}[t]={p_n^{(m)}[t]},\pmb{f}[t]={f_n^{(m)}[t]}$。</p>
</blockquote>
<ul>
<li><p>约束C1保证计算任务的延迟要求。C2表示任务卸载百分比。C3-C5表示每个UE最多可以分配一个信道，每个区域中的每个信道最多分配给一个UE。C6表示每个UE的传输功率约束。C7表示分配给每个UE的计算资源不能超过每个单元中MEC服务器的可用资源。</p>
</li>
<li><p>使用DRL来解决此全局优化问题。</p>
</li>
</ul>
<h2 id="多智能体环境"><a href="#多智能体环境" class="headerlink" title="多智能体环境"></a>多智能体环境</h2><ul>
<li><p>让SBS当Agent，让其与环境交互。</p>
<ul>
<li><p>State：在现实环境中，让S[t]包含全局信道状态和所有SBS Agent的行为是不现实的，他们只能观测到部分环境。在本文中，Agent m能观测到每个信道上自身信号链路的本地信号增益、每个信道SBS m接收到的干扰功率，以及其覆盖的UE的任务配置。 <strong>$s_m[t]=\{\pmb{h}_m[t],\pmb{I}_m[t],\pmb{Task}_m[t]\}$</strong></p>
<ul>
<li>$\pmb{h}_m[t]={[h_{1,1}^{(m)}[t],…,[h_{1,K}^{(m)}[t]],…,[h_{|\mathcal{N}_m|,1}^{(m)}[t],…,h_{|\mathcal{N}_m|,K}^{(m)}[t]]}$ 表示n到m的在信道k上的瞬时增益。</li>
<li>$\pmb{I}_m[t]=[I_1^{(m)}[t],…,I_K^{(m)}[t]]$ 表示m接收到的干扰功率。</li>
<li>$\pmb{Task}_m[t]=[Task_1^{(m)}[t],…,Task_{|\mathcal{N}_m|}^{(m)}[t]]$ 表示m对UE的配置文件。</li>
</ul>
</li>
<li><p>Action Space:在每个时隙t中，Agent m根据其观测做出动作。其动作包括计算卸载决策、信道分配、上行链路功率控制和计算资源分配。 <strong>$a_m[t]=\{\pmb{\alpha}_m[t],\pmb{b}_m[t],\pmb{p}_m[t],\pmb{f}_m[t]\}$</strong></p>
<ul>
<li>$\pmb{\alpha}_m[t]=[\alpha_1^{(m)}[t],…,\alpha_{|\mathcal{N}_m|}^{(m)}[t]]$ 表示计算卸载决策。</li>
<li>$\pmb{b}_m[t]={[b_{1,1}^{(m)}[t],…,[b_{1,K}^{(m)}[t]],…,[b_{|\mathcal{N}_m|,1}^{(m)}[t],…,b_{|\mathcal{N}_m|,K}^{(m)}[t]]}$ 表示分配给每个UE用于将卸载任务传输给m的信道。</li>
<li>$\pmb{p}_m[t]=[p_1^{(m)}[t],…,p_{|\mathcal{N}_m|}^{(m)}[t]]$ 表示UE n上行链路的传输速率。当$p_n^{(m)}[t]=0$时表示此任务在本地计算。</li>
<li>$\pmb{f}_m[t]=[f_1^{(m)}[t],…,f_{|\mathcal{N}_m|}^{(m)}[t]]$ 表示SBS m分给每个UE n的计算资源，当$f_n^{(m)}[t]=0$时表示此任务在本地计算。</li>
</ul>
</li>
<li><p>Reward:目标是最小化能耗，定义即时能耗为：</p>
<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mtable columnalign="left" columnspacing="1em" rowspacing="4pt"><mtr><mtd><msubsup><mi>R</mi><mrow><mi>m</mi></mrow><mrow><mi>C</mi><mi>o</mi></mrow></msubsup><mo stretchy="false">[</mo><mi>t</mi><mo stretchy="false">]</mo></mtd></mtr><mtr><mtd><mo>=</mo><munder><mo data-mjx-texclass="OP">∑</mo><mrow><mi>m</mi><mo>∈</mo><mrow><mi data-mjx-variant="-tex-calligraphic" mathvariant="script">M</mi></mrow></mrow></munder><munder><mo data-mjx-texclass="OP">∑</mo><mrow><mi>n</mi><mo>∈</mo><msub><mrow><mi data-mjx-variant="-tex-calligraphic" mathvariant="script">N</mi></mrow><mrow><mi>m</mi></mrow></msub></mrow></munder><mrow data-mjx-texclass="INNER"><mo data-mjx-texclass="OPEN">(</mo><mo>−</mo><mi>L</mi><mi>o</mi><mi>c</mi><msubsup><mi>E</mi><mrow><mi>n</mi></mrow><mrow><mo stretchy="false">(</mo><mi>m</mi><mo stretchy="false">)</mo></mrow></msubsup><mo stretchy="false">[</mo><mi>t</mi><mo stretchy="false">]</mo><mo>−</mo><mi>O</mi><mi>f</mi><mi>f</mi><msubsup><mi>E</mi><mrow><mi>n</mi></mrow><mrow><mo stretchy="false">(</mo><mi>m</mi><mo stretchy="false">)</mo></mrow></msubsup><mo stretchy="false">[</mo><mi>t</mi><mo stretchy="false">]</mo><mo>+</mo><msubsup><mi>V</mi><mrow><mi>n</mi></mrow><mrow><mo stretchy="false">(</mo><mi>m</mi><mo stretchy="false">)</mo></mrow></msubsup><mo stretchy="false">[</mo><mi>t</mi><mo stretchy="false">]</mo><mo data-mjx-texclass="CLOSE">)</mo></mrow></mtd></mtr></mtable></math>

<blockquote>
<p>-总能耗+剩余时间</p>
</blockquote>
<p>其中$V_n^{(m)}[t]$为：</p>
<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><msubsup><mi>V</mi><mrow><mi>n</mi></mrow><mrow><mo stretchy="false">(</mo><mi>m</mi><mo stretchy="false">)</mo></mrow></msubsup><mo stretchy="false">[</mo><mi>t</mi><mo stretchy="false">]</mo><mo>=</mo><mi>G</mi><mrow data-mjx-texclass="INNER"><mo data-mjx-texclass="OPEN">(</mo><msubsup><mrow><mover><mi>T</mi><mo stretchy="false">~</mo></mover></mrow><mrow><mi>n</mi></mrow><mrow><mo stretchy="false">(</mo><mi>m</mi><mo stretchy="false">)</mo></mrow></msubsup><mo>−</mo><mo data-mjx-texclass="OP" movablelimits="true">max</mo><mrow data-mjx-texclass="INNER"><mo data-mjx-texclass="OPEN">{</mo><mi>Loc</mi><mo data-mjx-texclass="NONE">⁡</mo><msubsup><mi>T</mi><mrow><mi>n</mi></mrow><mrow><mo stretchy="false">(</mo><mi>m</mi><mo stretchy="false">)</mo></mrow></msubsup><mo stretchy="false">[</mo><mi>t</mi><mo stretchy="false">]</mo><mo>,</mo><mi>O</mi><mi>f</mi><mi>f</mi><msubsup><mi>T</mi><mrow><mi>n</mi></mrow><mrow><mo stretchy="false">(</mo><mi>m</mi><mo stretchy="false">)</mo></mrow></msubsup><mo stretchy="false">[</mo><mi>t</mi><mo stretchy="false">]</mo><mo data-mjx-texclass="CLOSE">}</mo></mrow><mo data-mjx-texclass="CLOSE">)</mo></mrow></math>

<blockquote>
<p>n可以忍受的最大延迟-max(本地计算时间，卸载边缘服务器的总计算时间)</p>
</blockquote>
<p>其中$G(x)$为分段函数。例如：当$x≥0$时，$G(x)=\beta$，其他情况$G(x)=x$.$V_n^{(m)}[t]$为一个常数，$\beta$，当计算任务的延迟达到要求时为受益，否则为负惩罚，绝对值等于延迟要求与执行计算任务所需时间的差值。在实践中，$\beta$为一个超参数，需要调优。agent的学习目标是使折扣奖励最大化，记为：</p>
<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><msub><mi>J</mi><mrow><mi>m</mi></mrow></msub><mo>=</mo><mrow><mi mathvariant="double-struck">E</mi></mrow><mrow data-mjx-texclass="INNER"><mo data-mjx-texclass="OPEN">(</mo><munderover><mo data-mjx-texclass="OP">∑</mo><mrow><mi>t</mi><mo>=</mo><mn>0</mn></mrow><mrow><mi mathvariant="normal">∞</mi></mrow></munderover><msup><mi>γ</mi><mrow><mi>t</mi></mrow></msup><msubsup><mi>R</mi><mrow><mi>m</mi></mrow><mrow><mi>C</mi><mi>o</mi></mrow></msubsup><mo stretchy="false">[</mo><mi>t</mi><mo stretchy="false">]</mo><mo data-mjx-texclass="CLOSE">)</mo></mrow></math>

</li>
</ul>
</li>
</ul>
<h2 id="基于MARL的计算卸载于资源分配方案"><a href="#基于MARL的计算卸载于资源分配方案" class="headerlink" title="基于MARL的计算卸载于资源分配方案"></a>基于MARL的计算卸载于资源分配方案</h2><p><img src = "
/Multi-Agent Deep Reinforcement Learning for Computation Offloading and Interference Coordination in Small Cell Networks/
图片2.png
" height = 600></p>
<ul>
<li>每个Agent都被建模为DDPG。Critic网络输出Q值帮助Actor训练，Actor网络输出动作。且都有一个target网络。</li>
<li>设Actor网络的集合为$\pmb{\mu}=\{\pmb{\mu}_1,…,\pmb{\mu}_M\}$,参数为$\pmb{\theta^{\mu}}=\{\theta_1^{\pmb{\mu}},…,\theta_M^{\pmb{\mu}}\}$。target actor网络为$\pmb{\mu^{‘}}_m$，参数为$\theta^{\pmb{\mu^{‘}}}_m$。</li>
<li>设critic网络的集合为$\pmb{Q}=\{Q_1,…,Q_M\}$,参数为$\pmb{\theta}^{Q}=\{\theta_1^{Q},…,\theta_M^{Q}\}$。target critic网络为$Q^{‘}_m$，参数为$\theta^{Q^{‘}}_m$。</li>
<li>replay buffer中存放的transition为 $(s[t],a[t],r[t],s[t+1])$ ，<br>其中$s[t]=\{s_1[t],…,s_M[t]\}, a[t]=\{a_1[t],…,a_M[t]\}, r[t]=\{r_1[t],…,r_M[t]\},r_m[t]=R_m^{Co}[t]$。</li>
<li>Loss Function:</li>
</ul>
<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><msub><mi>L</mi><mi>m</mi></msub><mo stretchy="false">(</mo><msubsup><mi>θ</mi><mi>m</mi><mi>Q</mi></msubsup><mo stretchy="false">)</mo><mo>=</mo><mfrac><mn>1</mn><mi>W</mi></mfrac><munderover><mo data-mjx-texclass="OP">∑</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mi>W</mi></munderover><mo stretchy="false">[</mo><msubsup><mi>y</mi><mi>m</mi><mi>j</mi></msubsup><mo>−</mo><msubsup><mi>Q</mi><mi>m</mi><mi>μ</mi></msubsup><mo stretchy="false">(</mo><msup><mi>s</mi><mi>j</mi></msup><mo>,</mo><msubsup><mi>a</mi><mn>1</mn><mi>j</mi></msubsup><mo>,</mo><mo>.</mo><mo>.</mo><mo>.</mo><mo>,</mo><msubsup><mi>a</mi><mi>M</mi><mi>j</mi></msubsup><mo stretchy="false">)</mo><msup><mo stretchy="false">]</mo><mn>2</mn></msup></math>

<blockquote>
<p>W为batch size，$y_m^j$为target critic估计的目标值，计算公式为：</p>
<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><msubsup><mi>y</mi><mi>m</mi><mi>j</mi></msubsup><mo>=</mo><msubsup><mi>r</mi><mi>m</mi><mi>j</mi></msubsup><mo>+</mo><mi>γ</mi><msubsup><mi>Q</mi><mi>m</mi><mrow><mrow><mpadded width="0"><msup><mi>μ</mi><mrow><msup><mi></mi><mo>′</mo></msup></mrow></msup></mpadded></mrow><mspace width="1px"></mspace><mrow><msup><mi>μ</mi><mrow><msup><mi></mi><mo>′</mo></msup></mrow></msup></mrow></mrow></msubsup><mo stretchy="false">(</mo><msup><mi>s</mi><mrow><msup><mi></mi><mo>′</mo></msup><mi>j</mi></mrow></msup><mo>,</mo><msubsup><mi>a</mi><mn>1</mn><mrow><msup><mi></mi><mo>′</mo></msup></mrow></msubsup><mo>,</mo><mo>.</mo><mo>.</mo><mo>.</mo><mo>,</mo><msubsup><mi>a</mi><mi>m</mi><mrow><msup><mi></mi><mo>′</mo></msup></mrow></msubsup><mo stretchy="false">)</mo><msub><mo stretchy="false">|</mo><mrow><msubsup><mi>a</mi><mi>k</mi><mrow><msup><mi></mi><mo>′</mo></msup></mrow></msubsup><mo>=</mo><mrow><mpadded width="0"><mi>μ</mi></mpadded></mrow><mspace width="1px"></mspace><msubsup><mrow><mi>μ</mi></mrow><mi>k</mi><mrow><msup><mi></mi><mo>′</mo></msup></mrow></msubsup><mo stretchy="false">(</mo><msubsup><mi>s</mi><mi>k</mi><mi>j</mi></msubsup><mo stretchy="false">)</mo></mrow></msub></math>

</blockquote>
<ul>
<li><p>critic网络更新:</p>
<center>$\theta _m^Q \gets \theta _m^Q - \delta \nabla _{\theta _m^Q}L(\theta _m^Q)$</center>
</li>
<li><p>Actor网络使用policy gradient方式更新：</p>
<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mtable columnalign="left" columnspacing="1em" rowspacing="4pt"><mtr><mtd><msub><mi mathvariant="normal">∇</mi><mrow><msubsup><mi>θ</mi><mrow><mi>m</mi></mrow><mrow><mi>μ</mi></mrow></msubsup></mrow></msub><msub><mi>J</mi><mrow><mi>m</mi></mrow></msub><mrow data-mjx-texclass="INNER"><mo data-mjx-texclass="OPEN">(</mo><msubsup><mi>θ</mi><mrow><mi>m</mi></mrow><mrow><mi mathvariant="bold-italic">μ</mi></mrow></msubsup><mo data-mjx-texclass="CLOSE">)</mo></mrow><mo>=</mo></mtd></mtr><mtr><mtd><msub><mrow data-mjx-texclass="INNER"><mo data-mjx-texclass="OPEN" fence="true" stretchy="true" symmetric="true"></mo><mfrac><mn>1</mn><mi>W</mi></mfrac><munderover><mo data-mjx-texclass="OP">∑</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mrow><mi>W</mi></mrow></munderover><msub><mi mathvariant="normal">∇</mi><mrow><msubsup><mi>θ</mi><mrow><mi>m</mi></mrow><mrow><mi>μ</mi></mrow></msubsup></mrow></msub><mi mathvariant="bold-italic">μ</mi><mrow data-mjx-texclass="INNER"><mo data-mjx-texclass="OPEN">(</mo><msubsup><mi>s</mi><mrow><mi>m</mi></mrow><mrow><mi>j</mi></mrow></msubsup><mo data-mjx-texclass="CLOSE">)</mo></mrow><msub><mi mathvariant="normal">∇</mi><mrow><msub><mi>a</mi><mrow><mi>m</mi></mrow></msub></mrow></msub><msubsup><mi>Q</mi><mrow><mi>m</mi></mrow><mrow><mi mathvariant="bold-italic">μ</mi></mrow></msubsup><mrow data-mjx-texclass="INNER"><mo data-mjx-texclass="OPEN">(</mo><msup><mrow><mi mathvariant="bold">s</mi></mrow><mrow><mi>j</mi></mrow></msup><mo>,</mo><msubsup><mi>a</mi><mrow><mn>1</mn></mrow><mrow><mi>j</mi></mrow></msubsup><mo>,</mo><mo>.</mo><mo>,</mo><msub><mi>a</mi><mrow><mi>m</mi></mrow></msub><mo>,</mo><mo>.</mo><mo>,</mo><msubsup><mi>a</mi><mrow><mi>M</mi></mrow><mrow><mi>j</mi></mrow></msubsup><mo data-mjx-texclass="CLOSE">)</mo></mrow><mo data-mjx-texclass="CLOSE">|</mo></mrow><mrow><msub><mi>a</mi><mrow><mi>m</mi></mrow></msub><mo>=</mo><mi mathvariant="bold-italic">μ</mi><mrow data-mjx-texclass="INNER"><mo data-mjx-texclass="OPEN">(</mo><msubsup><mi>s</mi><mrow><mi>m</mi></mrow><mrow><mi>j</mi></mrow></msubsup><mo data-mjx-texclass="CLOSE">)</mo></mrow></mrow></msub></mtd></mtr></mtable></math>

</li>
</ul>
<p>$\theta^{\pmb{\mu}}_m$参数更新：</p>
<center>$\theta ^\mu _m \gets  \theta ^\mu _m - \delta \nabla _{\theta ^\mu _m} J(\theta ^\mu _m)$</center>

<ul>
<li>target网络参数更新<center>$\theta^{Q^{'}}_m = \tau \theta^Q_m  + (1 - \tau)\theta^{Q^{'}}_m $</center>
<center>$\theta^{\pmb{\mu} ^{'}}_m = \tau \theta^{\pmb{\mu}}_m  + (1 - \tau)\theta^{\pmb{\mu} ^{'}}_m  $</center>

</li>
</ul>
<h2 id="MADRL算法"><a href="#MADRL算法" class="headerlink" title="MADRL算法"></a>MADRL算法</h2><p><img src = "
/Multi-Agent Deep Reinforcement Learning for Computation Offloading and Interference Coordination in Small Cell Networks/
图片3.png
" height = 700></p>
<hr>
<h1 id="联邦DRL计算卸载"><a href="#联邦DRL计算卸载" class="headerlink" title="联邦DRL计算卸载"></a>联邦DRL计算卸载</h1><h2 id="问题表述"><a href="#问题表述" class="headerlink" title="问题表述"></a>问题表述</h2><p>在提出的MARL算法中，需要所有的Agent的观察和动作的全局信息来训练Actor和Critic网络，因此Agent之间需要交换他们的局部信息，这回导致额外的开销。为了减少开销，本文设计了一种用于联邦计算卸载和资源分配的算法。</p>
<ul>
<li>全局优化问题P0可以分解为M个子问题，每个子问题对应一个小基站。以区域m为例，给定$I_k^{(m)}[t],k \in \mathcal{K}$,局部优化子问题可以公式化为：</li>
</ul>
<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mtable displaystyle="true" columnalign="right left" columnspacing="0em" rowspacing="3pt"><mtr><mtd></mtd><mtd><mi>P</mi><mn>1</mn><mo>:</mo><munder><mo data-mjx-texclass="OP" movablelimits="true">min</mo><mrow><mrow><mpadded width="0"><mi>α</mi></mpadded></mrow><mspace width="1px"></mspace><msub><mrow><mi>α</mi></mrow><mi>m</mi></msub><mo stretchy="false">[</mo><mi>t</mi><mo stretchy="false">]</mo><mo>,</mo><mrow><mpadded width="0"><mi>b</mi></mpadded></mrow><mspace width="1px"></mspace><msub><mrow><mi>b</mi></mrow><mi>m</mi></msub><mo stretchy="false">[</mo><mi>t</mi><mo stretchy="false">]</mo><mo>,</mo><mrow><mpadded width="0"><mi>p</mi></mpadded></mrow><mspace width="1px"></mspace><msub><mrow><mi>p</mi></mrow><mi>m</mi></msub><mo stretchy="false">[</mo><mi>t</mi><mo stretchy="false">]</mo><mo>,</mo><mrow><mpadded width="0"><mi>f</mi></mpadded></mrow><mspace width="1px"></mspace><msub><mrow><mi>f</mi></mrow><mi>m</mi></msub><mo stretchy="false">[</mo><mi>t</mi><mo stretchy="false">]</mo></mrow></munder><munder><mo data-mjx-texclass="OP">∑</mo><mi>n</mi></munder><mo stretchy="false">(</mo><mi>L</mi><mi>o</mi><mi>c</mi><msubsup><mi>E</mi><mi>n</mi><mrow><mo stretchy="false">(</mo><mi>m</mi><mo stretchy="false">)</mo></mrow></msubsup><mo stretchy="false">[</mo><mi>t</mi><mo stretchy="false">]</mo><mo>+</mo><mi>O</mi><mi>f</mi><mi>f</mi><msubsup><mi>E</mi><mi>n</mi><mrow><mo stretchy="false">(</mo><mi>m</mi><mo stretchy="false">)</mo></mrow></msubsup><mo stretchy="false">[</mo><mi>t</mi><mo stretchy="false">]</mo><mo stretchy="false">)</mo></mtd></mtr><mtr><mtd></mtd><mtd><mi>s</mi><mo>.</mo><mi>t</mi><mo>.</mo></mtd></mtr><mtr><mtd></mtd><mtd><mi>C</mi><mn>1</mn><mo>−</mo><mi>C</mi><mn>7</mn><mi>i</mi><mi>n</mi><mo stretchy="false">(</mo><mn>11</mn><mo stretchy="false">)</mo></mtd></mtr></mtable></math>

<blockquote>
<p>$\pmb{\alpha}_m[t]=\{\alpha_n^{(m)}[t]\}, \pmb{b}_m[t]=\{b_{n,k}^{(m)}[t]\},\pmb{p}_m[t]=\{p_n^{(m)}[t]\},\pmb{f}_m[t]=\{f_n^{(m)}[t]\}, n \in \mathcal{N}_m,k \in \mathcal{K}$ 。注意P1也是一个MINLP问题，目标是最小化单元的能耗，而不是所有单元的总能耗。此外，所有的约束只与它本身有关。</p>
</blockquote>
<ul>
<li>可以使用分布式联邦学习来提高各个本地DRL模型的训练性能，而无需集中训练数据。</li>
</ul>
<h2 id="联邦MARL框架"><a href="#联邦MARL框架" class="headerlink" title="联邦MARL框架"></a>联邦MARL框架</h2><p><img src = "
/Multi-Agent Deep Reinforcement Learning for Computation Offloading and Interference Coordination in Small Cell Networks/
图片4.png
" height = 600></p>
<ul>
<li><p>如图所示，每个SBS运行一个本地DDPG模型。根据P1，每个SBS Agent的目标是在满足延迟需求的同时，最小化能耗。m在时隙t中的奖励$R_m^{Fl}[t]$定义为：</p>
<center>$R_m^{Fl}[t]=\sum _{n \in \mathcal{N}_m}(-LocE_n^{(m)}[t]-OffE_n^{(m)}[t]+V_n^{(m)}[t])$</center>
</li>
<li><p>对于Agent，我们定义Actor网络为$\pmb{\mu}_m$，参数为$\theta^{\pmb{\mu^{‘}}}_m$。critic网络为$Q_m$,参数为$\theta^Q_m$。同样有一个replay buffer,transition为$(s_m[t],a_m[t],R_m^{Gl}[t],s_m[t+1])$。</p>
</li>
<li><p>Loss Function：</p>
<center>$L_m^{Fl}(\theta _m^Q)=\frac{1}{W_m}\sum _{j=1}^{W_m}[y_m^j-Q_m^{\pmb{\mu}}(s_n^j,a_m^j)] ^2$</center>

</li>
</ul>
<blockquote>
<p>$W_m$为batch size，$y_m^j = R_m^{Fl}+\gamma Q_m^{\pmb{\mu^{‘}}}(s_n^{‘j},a_m^{‘j})|_{a_m^{‘j}=\pmb{\mu}^{‘}_m (s_n^{‘j})}$ 是target critic的目标值。</p>
</blockquote>
<ul>
<li>actor网络使用policy gradient更新：<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mtable columnalign="left" columnspacing="1em" rowspacing="4pt"><mtr><mtd><msub><mi mathvariant="normal">∇</mi><mrow><msubsup><mi>θ</mi><mrow><mi>m</mi></mrow><mrow><mi>μ</mi></mrow></msubsup></mrow></msub><msubsup><mi>J</mi><mrow><mi>m</mi></mrow><mrow><mi>F</mi><mi>l</mi></mrow></msubsup><mrow data-mjx-texclass="INNER"><mo data-mjx-texclass="OPEN">(</mo><msubsup><mi>θ</mi><mrow><mi>m</mi></mrow><mrow><mi mathvariant="bold-italic">μ</mi></mrow></msubsup><mo data-mjx-texclass="CLOSE">)</mo></mrow></mtd></mtr><mtr><mtd><mo>=</mo><msub><mrow data-mjx-texclass="INNER"><mo data-mjx-texclass="OPEN" fence="true" stretchy="true" symmetric="true"></mo><mfrac><mn>1</mn><msub><mi>W</mi><mrow><mi>m</mi></mrow></msub></mfrac><munderover><mo data-mjx-texclass="OP">∑</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mrow><msub><mi>W</mi><mrow><mi>m</mi></mrow></msub></mrow></munderover><msub><mi mathvariant="normal">∇</mi><mrow><msubsup><mi>θ</mi><mrow><mi>m</mi></mrow><mrow><mi>μ</mi></mrow></msubsup></mrow></msub><mi mathvariant="bold-italic">μ</mi><mrow data-mjx-texclass="INNER"><mo data-mjx-texclass="OPEN">(</mo><msubsup><mi>s</mi><mrow><mi>m</mi></mrow><mrow><mi>j</mi></mrow></msubsup><mo data-mjx-texclass="CLOSE">)</mo></mrow><msub><mi mathvariant="normal">∇</mi><mrow><msub><mi>a</mi><mrow><mi>m</mi></mrow></msub></mrow></msub><msubsup><mi>Q</mi><mrow><mi>m</mi></mrow><mrow><mi>μ</mi></mrow></msubsup><mrow data-mjx-texclass="INNER"><mo data-mjx-texclass="OPEN">(</mo><msubsup><mi>s</mi><mrow><mi>m</mi></mrow><mrow><mi>j</mi></mrow></msubsup><mo>,</mo><msubsup><mi>a</mi><mrow><mi>m</mi></mrow><mrow><mi>j</mi></mrow></msubsup><mo data-mjx-texclass="CLOSE">)</mo></mrow><mo data-mjx-texclass="CLOSE">|</mo></mrow><mrow><msubsup><mi>a</mi><mrow><mi>m</mi></mrow><mrow><mi>j</mi></mrow></msubsup><mo>=</mo><mi>μ</mi><mrow data-mjx-texclass="INNER"><mo data-mjx-texclass="OPEN">(</mo><msubsup><mi>s</mi><mrow><mi>m</mi></mrow><mrow><mi>j</mi></mrow></msubsup><mo data-mjx-texclass="CLOSE">)</mo></mrow></mrow></msub></mtd></mtr></mtable></math>

</li>
</ul>
<h2 id="算法"><a href="#算法" class="headerlink" title="算法"></a>算法</h2><p><img src = "
/Multi-Agent Deep Reinforcement Learning for Computation Offloading and Interference Coordination in Small Cell Networks/
图片5.png
" height = 900></p>
<ul>
<li><p>在该算法中，联邦学习过程控制了多轮协调。在每一轮中，每一个Agent经过了多次训练，根据其本地数据独立训练其本地模型，而无需了解其他Agent的观察和动作。因此Agent之间没有信息交换，系统的通信开销也显著降低。</p>
</li>
<li><p>在每一轮进行多次训练后，每个Agent将其本地模型的权重上传到MBS（宏基站）就行聚合。联邦平均采用基于小批量的随机梯度下降法：</p>
<center>$\theta^r \gets \sum_m \frac{W_m}{\tilde{W}}\theta_m^r$</center>

</li>
</ul>
<blockquote>
<p>$\tilde{W} = \sum_m W_m$：所有Agent的batch size之和。</p>
</blockquote>
<ul>
<li><p>然后，协调器将平均的全局模型分发回所有Agent让其相应地更新本地模型。或者，模型聚合可以在每个Agent的本地执行。特别是，Agent之间可以通过单元间专用控制通道来交换其模型参数。每个Agent获取所有Agent的模型参数并执行相同的联邦平均操作，这样不同的Age他就可以获得与MBS全局执行的聚合模型相同的模型。此外，相邻的小基站可能经历类似的环境观测，相邻小基站Agent可以聚类成一个合作组来执行模型聚合，降低了通信开销。</p>
</li>
<li><p>尽管actor网络与MARL的网络结构相同，但critic网络的输入大小显著减小，因为只考虑局部状态，而不是所有Agent的状态。因此与MARL相比，联邦DRL的算法复杂度显著降低。</p>
</li>
</ul>
<h1 id="原文地址"><a href="#原文地址" class="headerlink" title="原文地址"></a>原文地址</h1><p><a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=9485089">Huang, Xiaoyan, et al. “Multi-agent deep reinforcement learning for computation offloading and interference coordination in small cell networks.” IEEE Transactions on Vehicular Technology 70.9 (2021): 9282-9293.</a></p>

    </div>

    
    
    

    <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/" rel="tag"># 强化学习</a>
              <a href="/tags/%E6%96%87%E7%8C%AE%E9%98%85%E8%AF%BB/" rel="tag"># 文献阅读</a>
              <a href="/tags/%E8%BE%B9%E7%BC%98%E8%AE%A1%E7%AE%97/" rel="tag"># 边缘计算</a>
              <a href="/tags/%E8%AE%A1%E7%AE%97%E5%8D%B8%E8%BD%BD/" rel="tag"># 计算卸载</a>
              <a href="/tags/%E8%81%94%E9%82%A6%E5%AD%A6%E4%B9%A0/" rel="tag"># 联邦学习</a>
          </div>

        

          <div class="post-nav">
            <div class="post-nav-item">
                <a href="/2022/04/26/%5B%E6%96%87%E7%8C%AE%E9%98%85%E8%AF%BB%5DDeep%20Reinforcement%20Learning%20for%20Task%20Offloading%20in%20Mobile%20Edge%20Computing%20Systems%E8%AF%91%E6%96%87:%20%E7%A7%BB%E5%8A%A8%E8%BE%B9%E7%BC%98%E8%AE%A1%E7%AE%97%E7%B3%BB%E7%BB%9F%E4%B8%AD%E4%BB%BB%E5%8A%A1%E5%8D%B8%E8%BD%BD%E7%9A%84%E6%B7%B1%E5%BA%A6%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/" rel="prev" title="[文献阅读] Deep Reinforcement Learning for Task Offloading in Mobile Edge Computing Systems 移动边缘计算系统中任务卸载的深度强化学习">
                  <i class="fa fa-chevron-left"></i> [文献阅读] Deep Reinforcement Learning for Task Offloading in Mobile Edge Computing Systems 移动边缘计算系统中任务卸载的深度强化学习
                </a>
            </div>
            <div class="post-nav-item">
            </div>
          </div>
    </footer>
  </article>
</div>






    <div class="comments" id="lv-container" data-id="city" data-uid="MTAyMC81NTkwNy8zMjM3MQ=="></div>
</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">


<div class="copyright">
  &copy; 
  <span itemprop="copyrightYear">2022</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Ciallo～(∠・ω< )⌒☆​</span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io/" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.js.org/pisces/" rel="noopener" target="_blank">NexT.Pisces</a> 强力驱动
  </div>

    </div>
  </footer>

  
  <script src="https://cdn.jsdelivr.net/npm/animejs@3.2.1/lib/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous"></script>
<script src="/js/comments.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/next-boot.js"></script>

  





  




  

  <script class="next-config" data-name="enableMath" type="application/json">true</script><script class="next-config" data-name="mathjax" type="application/json">{"enable":true,"mhchem":true,"tags":"none","js":{"url":"https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js","integrity":"sha256-r+3itOMtGGjap0x+10hu6jW/gZCzxHsoKrOd7gyRSGY="}}</script>
<script src="/js/third-party/math/mathjax.js"></script>


<script src="/js/third-party/comments/livere.js"></script>

</body>
</html>
